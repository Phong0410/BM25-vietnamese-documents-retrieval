{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OkapiBM25-health_news.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "66xWOTmrpyiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d38b9d2-cb0a-44e5-9c05-035e1e015d84"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import string\n",
        "import json\n",
        "import re\n",
        "# from multiprocessing import Pool, cpu_count\n",
        "import string\n",
        "import re\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import time\n",
        "!pip install underthesea"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-1.3.4-py3-none-any.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 6.0 MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "  Downloading Unidecode-1.3.3-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Collecting underthesea-core==0.0.4_alpha.10\n",
            "  Downloading underthesea_core-0.0.4_alpha.10-cp37-cp37m-manylinux2010_x86_64.whl (581 kB)\n",
            "\u001b[K     |████████████████████████████████| 581 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Installing collected packages: unidecode, underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.7 underthesea-1.3.4 underthesea-core-0.0.4a10 unidecode-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BM25 algorithms"
      ],
      "metadata": {
        "id": "WUlJRa02ZM7D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8X6x_h1BlXI"
      },
      "source": [
        "# BM25 algorithms from rank_bm25\n",
        "\n",
        "class BM25:\n",
        "    def __init__(self, corpus, tokenizer=None):\n",
        "        self.corpus_size = len(corpus)\n",
        "        self.avgdl = 0\n",
        "        self.doc_freqs = []\n",
        "        self.idf = {}\n",
        "        self.doc_len = []\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        if tokenizer:\n",
        "            corpus = self._tokenize_corpus(corpus)\n",
        "\n",
        "        nd = self._initialize(corpus)\n",
        "        self._calc_idf(nd)\n",
        "\n",
        "    def _initialize(self, corpus):\n",
        "        nd = {}  # word -> number of documents with word\n",
        "        num_doc = 0\n",
        "        for document in corpus:\n",
        "            self.doc_len.append(len(document))\n",
        "            num_doc += len(document)\n",
        "\n",
        "            frequencies = {}\n",
        "            for word in document:\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 0\n",
        "                frequencies[word] += 1\n",
        "            self.doc_freqs.append(frequencies)\n",
        "\n",
        "            for word, freq in frequencies.items():\n",
        "                try:\n",
        "                    nd[word]+=1\n",
        "                except KeyError:\n",
        "                    nd[word] = 1\n",
        "\n",
        "        self.avgdl = num_doc / self.corpus_size\n",
        "        return nd\n",
        "\n",
        "    def _tokenize_corpus(self, corpus):\n",
        "        pool = Pool(cpu_count())\n",
        "        tokenized_corpus = pool.map(self.tokenizer, corpus)\n",
        "        return tokenized_corpus\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_batch_scores(self, query, doc_ids):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_top_n(self, query, documents, n=5):\n",
        "        assert self.corpus_size == len(documents)\n",
        "        scores = self.get_scores(query)\n",
        "        top_n = np.argsort(scores)[::-1][:n]\n",
        "        return [documents[i] for i in top_n]\n",
        "\n",
        "\n",
        "class BM25Okapi(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        idf_sum = 0\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score\n",
        "\n",
        "class BM25Robertson(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        idf_sum = 0\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score\n",
        "\n",
        "class BM25ATIRE(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.epsilon = epsilon\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        idf_sum = 0\n",
        "        negative_idfs = []\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size) - math.log(freq)\n",
        "            self.idf[word] = idf\n",
        "            idf_sum += idf\n",
        "            if idf < 0:\n",
        "                negative_idfs.append(word)\n",
        "        self.average_idf = idf_sum / len(self.idf)\n",
        "\n",
        "        eps = self.epsilon * self.average_idf\n",
        "        for word in negative_idfs:\n",
        "            self.idf[word] = eps\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
        "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
        "        return score\n",
        "\n",
        "class BM25L(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=0.5):\n",
        "        # Algorithm specific parameters\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.delta = delta\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log(self.corpus_size + 1) - math.log(freq + 0.5)\n",
        "            self.idf[word] = idf\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            ctd = q_freq / (1 - self.b + self.b * doc_len / self.avgdl)\n",
        "            score += (self.idf.get(q) or 0) * q_freq * (self.k1 + 1) * (ctd + self.delta) / \\\n",
        "                     (self.k1 + ctd + self.delta)\n",
        "        return score\n",
        "\n",
        "class BM25Plus(BM25):\n",
        "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, delta=1):\n",
        "        # Algorithm specific parameters\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.delta = delta\n",
        "        super().__init__(corpus, tokenizer)\n",
        "\n",
        "    def _calc_idf(self, nd):\n",
        "        for word, freq in nd.items():\n",
        "            idf = math.log((self.corpus_size + 1) / freq)\n",
        "            self.idf[word] = idf\n",
        "\n",
        "    def get_scores(self, query):\n",
        "        score = np.zeros(self.corpus_size)\n",
        "        doc_len = np.array(self.doc_len)\n",
        "        for q in query:\n",
        "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
        "            score += (self.idf.get(q) or 0) * (self.delta + (q_freq * (self.k1 + 1)) /\n",
        "                                               (self.k1 * (1 - self.b + self.b * doc_len / self.avgdl) + q_freq))\n",
        "        return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload dataset\n",
        "link: https://www.kaggle.com/yuiikin/vietnamese-vnexpress-news"
      ],
      "metadata": {
        "id": "Axe74w2ZZWXS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJMVKenPEda4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "98605740-bfe5-43ba-9d11-1348e2f1571d"
      },
      "source": [
        "#upload texts.json and create corpuses\n",
        "from google.colab import files\n",
        "uploaded=files.upload() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8bf4ae70-c382-4449-a099-7a731fb7527c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8bf4ae70-c382-4449-a099-7a731fb7527c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving health_news.csv to health_news.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('health_news.csv')\n",
        "data = df['content']\n",
        "\n",
        "docs = data.values.tolist()"
      ],
      "metadata": {
        "id": "k7CbnZDsanDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from underthesea import word_tokenize\n",
        "\n",
        "corpus = [doc.translate(str.maketrans('','', string.punctuation)) for doc in docs]\n",
        "corpus = [word_tokenize(sentence) for sentence in corpus]"
      ],
      "metadata": {
        "id": "QXKYKjdea4QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_cased_corpus = []\n",
        "for sentence in corpus:\n",
        "  sentence = [word.lower() for word in sentence]\n",
        "  lower_cased_corpus.append(sentence)"
      ],
      "metadata": {
        "id": "Rc1_plo5bW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords processing (maybe unnecessary)"
      ],
      "metadata": {
        "id": "rhwgdqXffhwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vietnamese stopwords\n",
        "import gensim\n",
        "\n",
        "# prepare a empty frozenset for vietnamese stopwords\n",
        "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "all_stopwords = all_stopwords.difference(all_stopwords)\n",
        "\n",
        "# upload vietnamese stopwords\n",
        "vietnamese_stopwords_set=[]\n",
        "uploaded=files.upload()\n",
        "sample = open(\"vietnamese-stopwords.txt\",\"r\")\n",
        "for i, l in enumerate(sample):\n",
        "  l = l.replace('\\n', '')\n",
        "  vietnamese_stopwords_set.append(l)\n",
        "\n",
        "# add vietnamese stopwords to frozenset\n",
        "stopwords = all_stopwords.union(set(vietnamese_stopwords_set))"
      ],
      "metadata": {
        "id": "uu7WNYN3e4LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_cased_corpus_without_sw = []\n",
        "for doc in lower_cased_corpus:\n",
        "  doc = [word for word in doc if not word in stopwords]\n",
        "  lower_cased_corpus_without_sw.append(doc)"
      ],
      "metadata": {
        "id": "v5TlY3Wxe1Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus indexing"
      ],
      "metadata": {
        "id": "9aDIAiDdfr5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25okapi = BM25Okapi(lower_cased_corpus)\n",
        "bm25robertson = BM25Robertson(lower_cased_corpus)\n",
        "bm25atire = BM25ATIRE(lower_cased_corpus)\n",
        "bm25l = BM25L(lower_cased_corpus)\n",
        "bm25plus = BM25Plus(lower_cased_corpus)"
      ],
      "metadata": {
        "id": "w9KQ2GaOcfsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "OxbjPdADfuwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Làm sao để giảm cân?\"\n",
        "print(\"Query:\")\n",
        "print(query, \"\\n\")\n",
        "\n",
        "query = query.translate(str.maketrans('', '', string.punctuation))\n",
        "query = word_tokenize(query)\n",
        "lower_cased_query = [word.lower() for word in query]\n",
        "\n",
        "result_docs = bm25l.get_top_n(lower_cased_query, docs, n = 10)\n",
        "print(\"Document:\")\n",
        "\n",
        "print(result_docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwOoOSVJdCDj",
        "outputId": "1f000c39-ce21-47f6-884a-9b276b626610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "Làm sao để giảm cân? \n",
            "\n",
            "Document:\n",
            "Những chiếc váy với thiết kế bó sát cơ thể làm tôn lên thân hình thon thả và số đo 3 vòng 92-67-89 cm. Mở chiếc điện thoại, cô gái nở nụ cười tươi và tạo hình nhiều tư thế khác nhau. Trước đây do thân hình mũm mĩm, Linh chỉ mặc quần áo tối màu vì như vậy nhìn cơ thể sẽ gầy hơn. Tuy nhiên giờ đây, cô có thể tự do mặc những chiếc váy mình thích mà không còn cảm thấy tự ti như trước.\n",
            "\"Từ rất lâu rồi, mình ao ước có được thân hình thon gọn để mặc váy xinh giống các bạn. Cuối cùng thì ngày này đã đến\", Linh nói. Ước mơ tưởng chừng có gì to tát nhưng lại là điều đeo bám trong cô gái nhiều năm.\n",
            "Cất những bộ váy vào tủ, Linh lấy ra những bức hình hồi học phổ thông. Bức ảnh mặc áo dài ngày bế giảng năm lớp 12 là một minh chứng rõ nét cho thể hình mũm mĩm, thừa cân. Đây là lúc cân nặng của cô đạt đỉnh điểm 64 kg, trong khi chiều cao khiêm tốn 1,55 m. Dù cố gắng trang điểm xinh đẹp nhưng cô gái vẫn không thể tự tin khi chụp ảnh cùng các bạn.\n",
            "\"Béo cũng đáng yêu\", cầm bức ảnh, Linh nói. \"Nhưng vì mình đã mất đi điểm hấp dẫn chính là sự tự tin và năng động, nên ngày đó mình luôn trong trạng thái tiêu cực, cảm thấy bản thân xấu xí\".\n",
            "Linh kể, từ nhỏ, cô rất thích ăn uống, đặc biệt là thịt, thực phẩm chiên rán và các đồ ăn vặt, vậy nên 'ai rủ đi đâu là đi đấy'. \"Mình ý thức được mình béo, nhưng mình không thể ngừng ăn, mặc kệ mọi người cười chê đả kích như thế nào\", Linh chia sẻ. Cô gái cho rằng do chuyển cấp, do áp lực học hành, thi cử... nên phải ăn nhiều, phớt lờ mọi sự thay đổi trong cơ thể.\n",
            "Lên đại học, Linh bắt đầu tự tìm hiểu các phương pháp giảm cân như uống các loại trà, nhịn ăn, ăn yến mạch thay thế, bạn bè rủ đi liên hoan cũng nhất quyết không đi. Song, \"mọi thứ không đơn giản\". Khoảng vài tuần đầu, Linh giảm được vài kg, tuy nhiên do nhịn ăn không khoa học, Linh thường xuyên trong tình trạng mệt mỏi, thiếu sức sống, da xanh xao do thiếu ngủ. Có lần khi đang ngồi trong lớp học, cô gục xuống bàn, suýt ngất vì cơ thiếu năng lượng. Linh được bổ sung dinh dưỡng ngay sau đó, cân nặng nhanh chóng lên trên 60 kg. \n",
            "Chán nản, cô gái nhận ra rằng cứ cố gắng mà không có mục đích cụ thể, không có lộ trình rõ ràng thì sẽ chẳng đi tới đâu. Suốt thời gian dài, Linh luôn trong trạng thái bế tắc, lúc thì ăn rất nhiều, lúc thì cả ngày chẳng thèm ăn.\n",
            "\"Mình giảm cân thất bại bởi vì phương pháp quá cứng nhắc. Bản thân vốn không có ý chí sắt đá của một vận động viên thể hình hay một người mẫu, vậy mà mình cứ ép mình vào khuôn khổ, phá vỡ quy tắc sinh hoạt thường nhật\".\n",
            "Mỗi lúc thả lỏng, Linh ăn nhiều hơn gấp bội vì bị kìm hãm bấy lâu, cân nặng tăng vèo vèo, cô nhớ lại.\n",
            "\"Cho đến một ngày, đứa bạn thân mình nói rằng cứ béo mãi và tự ti mãi thế này thì muôn đời không có người yêu đâu, mình mới quyết tâm giảm cân nghiêm túc\", Linh cười nói.\n",
            "Diệu Linh bắt đầu tìm đến huấn luyện viên thể hình. Cô được lên lịch mỗi tuần 4 buổi tập với huấn luyện viên, mỗi buổi một tiếng. Ba nhóm bài tập cơ bản Linh áp dụng là push (các nhóm cơ đẩy: ngực, vai, tay sau), pull (các nhóm cơ kéo: lưng, tay trước) và legs (các nhóm cơ thân dưới). Với Linh, bài tập cụ thể chủ yếu là tập tạ, chạy bộ, deepthrust, plank, squat, lifting va cardio. Ngoài thời gian tập với huấn luyện viên, Linh tự đến phòng tập hai buổi chạy bộ nữa, mỗi buổi hai tiếng. Các bài tập này có tác dụng giảm mỡ thừa hiệu quả.\n",
            "\"Khi bạn tập có sự hướng dẫn của huấn luyện viên, bạn sẽ thấy khác hoàn toàn so với việc cùng một phương pháp đó mà bạn tập một mình. Huấn luyện viên sẽ đưa bạn vào khuôn khổ ngay lập tức và đó là điều hết sức cần thiết để giảm cân bước đầu\", Linh nói. \"Với nhiều người, giảm cân đơn giản cần nhịn ăn, cắt mỡ bụng... Song khi tập luyện, mình cảm thấy thể hình của mình đẹp ra từng ngày, giúp mình tự tin hơn về bản thân. Dần mình thấy thích tập, ngày nào không tập sẽ thấy thiếu thiếu\".\n",
            "Đi vào căn bếp, Linh giới thiệu thực phẩm hàng ngày. Gạo lứt, yến mạch, khoai lang, ức gà và trứng là những món cô gái hay ăn và gần như chỉ ăn như vậy trong thời kỳ giảm cân. \"Thú thực thời gian đầu, mình vừa ăn vừa khóc vì chán không chịu được\", cô gái chia sẻ. \n",
            "Linh cân đo sao cho tổng lượng nạp vào cơ thể mỗi ngày không quá 2.000 kcal. Trong hai tháng đầu, cô gái ăn yến mạch thay cơm, chế biến bằng cách lấy 1-2 thìa yến mạch, hâm nóng lên sau đó khuấy đều như cháo, ăn một ngày 2 bữa, mỗi bữa một bát con. Thời gian sau, Linh thay đổi sang cơm gạo lứt, ức gà, khoai lang, trứng luộc, lượng nhất định. Gạo lứt hay yến mạch là những thực phẩm giúp bạn có cảm giác no lâu, hạn chế năng lượng thu nạp vào cơ thể. \n",
            "\"Mình ăn những thực phẩm đó vì mình muốn giảm cân nhanh nhất có thể. Còn với giảm cân nói chung, cái quan trọng là bạn tính toán được lượng calo nạp vào, bất kể ăn gì\", Linh nói. \"Trong khoảng thời gian áp dụng chế độ ăn kiêng, mình không từ chối những bữa liên hoan từ bạn bè. Mình vẫn ăn thịt cá như bình thường, nhưng ăn ít đi, miễn sao không vượt quá lượng calo tiêu chuẩn, đó là chế độ ăn kiêng linh hoạt\". Trong giảm cân có khái niệm macro - tỷ lệ dinh dưỡng. Bạn không muốn ăn ức gà, khoai lang, hãy chọn loại thực phẩm mà bạn thích, chỉ cần có tỷ lệ dinh dưỡng phù hợp. Bạn đi làm về đã 9 giờ tối, cứ ăn đi, chỉ cần có tỷ lệ dinh dưỡng phù hợp.\n",
            "Những lúc đói, Linh uống nước lọc để như một cách ngăn chặn sự thèm ăn. Nước lọc không chứa calo, giúp cơ thể tỉnh táo hơn nhiều. Linh cũng cố gắng ngủ đủ giấc (7-8 tiếng một ngày). \"Trước đây vì ôn thi nên mình hay thức khuya, cơ thể bị thiếu ngủ. Lúc đó, cảm giác đói tăng lên rõ rệt làm mình càng ăn nhiều hơn\", Linh chia sẻ.\n",
            "Khi áp dụng thực đơn ăn uống nghiêm ngặt và tập luyện, Linh cân hàng ngày để theo dõi cân nặng, vào mỗi buổi sáng sau khi ngủ dậy, ghi lại con số cân nặng và tính trung bình cân nặng theo tuần, so sánh trung bình cân nặng tuần này với tuần trước đó. Cô gái nhận thấy, cân nặng giảm đều hàng tuần, từ đó càng có động lực để tiếp tục hành trình giảm cân. Sau hai tháng, cân nặng của Linh giảm từ 64 kg xuống ở mức 50,6 kg, Bốn tháng sau, cô gái giảm xuống còn 44 kg. \n",
            "\"Nhiều khi sự thay đổi về cơ thể thậm chí không được thể hiện trên bàn cân, bởi lẽ, khi tế bào mỡ bị hao hụt đi, nó sẽ lấy các phân tử nước thế chỗ vào, hoặc lượng cơ bắp của bạn tăng lên trong khi lượng mỡ đồng thời giảm đi, khiến cân nặng không thay đổi\", Linh chia sẻ. \"Vậy nên mình hay chụp ảnh để ghi lại hình ảnh cơ thể mình\". Cô thường chọn một địa điểm có ánh sáng cố định, sử dụng cùng một góc chụp, mỗi tuần ghi lại hình ảnh cơ thể mình một lần. \"Mặc dù tốn thêm chút công sức, nhưng nó giúp mình đánh giá khách quan hơn về tiến độ của bản thân, để đưa ra điều chỉnh thích hợp cho chế độ tập luyện, dinh dưỡng\".\n",
            "Từ khi cân nặng đạt 44 kg, Diệu Linh vẫn duy trì chế độ ăn kiêng linh hoạt để cơ thể không trở lại béo phì như trước. Diet Linh Hoạt - IIFYM (If It Fit Your Macros) là không kiêng bất cứ món nào, không giới hạn thời gian, quan trọng nhất là bạn học cách kiểm soát khẩu phần ăn của mình tùy theo mục tiêu. Tất nhiên, điều quan trọng nhất là phải luôn kiên trì.\n",
            "\"Trót ăn nhiều một bữa, đừng để nó biến thành một chuỗi ngày buông thả, hãy quay lại với chế độ ăn của mình ngay lập tức. Trót bỏ lỡ một buổi tập, đừng dùng nó làm cớ để nghỉ hẳn cả tuần, hãy quay lại với lịch tập bình thường ngay hôm sau. Dù hôm sau đứng lên cân có tăng hai hay ba kg, hãy quay lại ngay lập tức với kế hoạch mà mình đã đặt ra\", Linh nói.\n"
          ]
        }
      ]
    }
  ]
}